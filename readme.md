# üöÄ AI Data Engineer Bootcamp

> A comprehensive, hands-on bootcamp for mastering modern data engineering with AI integration, featuring Apache Airflow 3.0, LLMs, and production-ready architectures.

## üìã Table of Contents
- [Overview](#overview)
- [Prerequisites](#prerequisites)
- [Bootcamp Structure](#bootcamp-structure)
- [Module Details](#module-details)
- [Technology Stack](#technology-stack)
- [Getting Started](#getting-started)
- [Learning Path](#learning-path)

## üéØ Overview

This bootcamp provides a practical, project-based approach to becoming an AI-powered Data Engineer. You'll build real-world data pipelines, implement document intelligence systems, and master modern orchestration patterns using cutting-edge technologies.

### Key Features
- **Production-Ready Code**: All examples follow industry best practices
- **Progressive Complexity**: Each module builds upon previous concepts
- **AI-First Approach**: Integration of LLMs and machine learning throughout
- **Cloud-Native Architecture**: Designed for scalability and modern deployments

## üìö Prerequisites

### Required Knowledge
- Python programming (intermediate level)
- SQL fundamentals
- Basic understanding of REST APIs
- Command line proficiency

### Recommended Experience
- Docker basics
- Cloud services familiarity (AWS/GCP/Azure)
- Git version control

## üóÇÔ∏è Bootcamp Structure

The bootcamp is organized into progressive modules, each focusing on specific aspects of AI-powered data engineering:

```
ai-data-engineer-bootcamp/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ mod-1-document-extract/    
‚îÇ   ‚îú‚îÄ‚îÄ mod-2-data-pipeline/       
‚îÇ   ‚îú‚îÄ‚îÄ mod-3-streaming/           
‚îÇ   ‚îú‚îÄ‚îÄ mod-4-mlops/              
‚îÇ   ‚îî‚îÄ‚îÄ mod-5-governance/         
‚îú‚îÄ‚îÄ storage/                       
‚îú‚îÄ‚îÄ scripts/                       
‚îî‚îÄ‚îÄ docs/                         
```

## üì¶ Module Details

### Module 1: Document Intelligence & Extraction
**`src/mod-1-document-extract/`**

Learn to build intelligent document processing pipelines using Apache Airflow 3.0 and LLMs.

#### üéì What You'll Learn
- Apache Airflow 3.0 fundamentals and DAG development
- PDF processing and text extraction techniques
- LLM integration for structured data extraction
- Progressive optimization patterns (V1 ‚Üí V2 ‚Üí V3)
- Asset-based scheduling and event-driven architectures

#### üõ†Ô∏è Technologies
- Apache Airflow 3.0
- OpenAI GPT-4
- MinIO (S3-compatible storage)
- PostgreSQL
- Docker & Docker Compose
- Airflow AI SDK

#### üìÇ Key Components
```
mod-1-document-extract/
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îú‚îÄ‚îÄ uber-eats-inv-extractor-v1.py  
‚îÇ   ‚îú‚îÄ‚îÄ uber-eats-inv-extractor-v2.py  
‚îÇ   ‚îú‚îÄ‚îÄ uber-eats-inv-extractor-v3.py  
‚îÇ   ‚îî‚îÄ‚îÄ extract_invoices_intelligence_lang.py  
‚îú‚îÄ‚îÄ schemas/                            
‚îú‚îÄ‚îÄ docker-compose.yaml                 
‚îî‚îÄ‚îÄ requirements.txt                 
```

#### üí° Key Concepts
- **Evolution Pattern**: See how a simple DAG evolves into an optimized, production-ready pipeline
- **Cost Optimization**: Learn to reduce LLM API costs by 80% through batching
- **Modern Patterns**: Task groups, asset scheduling, context managers
- **Clean Code**: No inline comments, comprehensive docstrings

#### Performance Metrics
| Version | API Calls | Processing Time | Cost |
|---------|-----------|-----------------|------|
| V1 | 20 calls | ~10 min | $0.20 |
| V2 | 4 calls | ~4 min | $0.04 |
| V3 | 4 calls | ~4 min | $0.04 |

---

### üì¶ Module 2: Advanced Data Pipeline Patterns (Coming Soon)
**`src/mod-2-data-pipeline/`**

Master complex data pipeline patterns including CDC, SCD, and data quality frameworks.

#### üéì What You'll Learn
- Change Data Capture (CDC) implementation
- Slowly Changing Dimensions (SCD) strategies
- Data quality and validation frameworks
- Pipeline monitoring and alerting
- Error handling and recovery patterns

---

### üì¶ Module 3: Real-time Data Processing (Coming Soon)
**`src/mod-3-streaming/`**

Build real-time data processing systems with streaming architectures.

#### üéì What You'll Learn
- Apache Kafka integration with Airflow
- Stream processing patterns
- Real-time analytics pipelines
- Event-driven architectures
- Windowing and aggregation strategies

---

### üì¶ Module 4: MLOps & Model Deployment (Coming Soon)
**`src/mod-4-mlops/`**

Implement MLOps practices for model training, deployment, and monitoring.

#### üéì What You'll Learn
- ML pipeline orchestration
- Model versioning and registry
- A/B testing frameworks
- Model monitoring and drift detection
- Feature store integration

---

### üì¶ Module 5: Data Governance & Quality (Coming Soon)
**`src/mod-5-governance/`**

Establish robust data governance and quality assurance practices.

#### üéì What You'll Learn
- Data lineage tracking
- Privacy and compliance (GDPR, CCPA)
- Data cataloging and discovery
- Quality metrics and SLAs
- Access control and security

## üîß Technology Stack

### Core Technologies
- **Orchestration**: Apache Airflow 3.0
- **Language**: Python 3.11+
- **LLMs**: OpenAI GPT-4, Claude
- **Databases**: PostgreSQL, MongoDB
- **Storage**: MinIO (S3-compatible)
- **Streaming**: Apache Kafka
- **Containerization**: Docker & Kubernetes

### AI/ML Stack
- **Frameworks**: LangChain, Airflow AI SDK
- **Vector DB**: Pinecone, Weaviate
- **Monitoring**: Langfuse, Weights & Biases
- **Model Registry**: MLflow

## üöÄ Getting Started

### 1. Clone the Repository
```bash
git clone https://github.com/yourusername/ai-data-engineer-bootcamp.git
cd ai-data-engineer-bootcamp
```

### 2. Environment Setup
```bash
python -m venv venv
source venv/bin/activate

pip install -r requirements.txt
```

### 3. Start with Module 1
```bash
cd src/mod-1-document-extract

pip install -r requirements.txt

docker-compose up -d


open http://localhost:8080 
```

### 4. Configure Connections
Set up required connections in Airflow UI:
- `minio_default`: MinIO connection
- `openai_default`: OpenAI API
- `invoice_db`: PostgreSQL

## Learning Path

### Recommended Progression

```mermaid
graph LR
    A[Module 1: Document Extraction] --> B[Module 2: Pipeline Patterns]
    B --> C[Module 3: Streaming]
    C --> D[Module 4: MLOps]
    D --> E[Module 5: Governance]
    
    style A fill:#4CAF50
    style B fill:#2196F3
    style C fill:#FF9800
    style D fill:#9C27B0
    style E fill:#F44336
```

## üéØ Learning Objectives

By completing this bootcamp, you will be able to:

1. **Design and implement** production-ready data pipelines using Apache Airflow 3.0
2. **Integrate LLMs** for intelligent data processing and extraction
3. **Optimize costs** through efficient batching and resource management
4. **Build event-driven** architectures with asset-based scheduling
5. **Implement MLOps** practices for model deployment and monitoring
6. **Establish governance** frameworks for data quality and compliance
7. **Deploy scalable** solutions using cloud-native technologies
8. **Apply best practices** for code organization and documentation

## ü§ù Contributing

Contributions are welcome! Please read our [Contributing Guidelines](CONTRIBUTING.md) before submitting PRs.

## üôè Acknowledgments

- Apache Airflow community for the amazing orchestration platform
- OpenAI for GPT models and AI capabilities
- Astronomer for Airflow AI SDK and best practices
- All contributors and learners who make this bootcamp better

## üì¨ Contact & Support

- **Issues**: [GitHub Issues](https://github.com/yourusername/ai-data-engineer-bootcamp/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/ai-data-engineer-bootcamp/discussions)
- **Email**: your.email@example.com

---

<p align="center">
  <strong>üöÄ Start your journey to becoming an AI-powered Data Engineer today!</strong>
</p>

<p align="center">
  Made with ‚ù§Ô∏è by the AI Data Engineering Community
</p>